<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2022/08/18/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very
first post. Check <a href="https://hexo.io/docs/">documentation</a> for
more info. If you get any problems when using Hexo, you can find the
answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or
you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>放学后</title>
    <url>/2022/08/27/%E6%94%BE%E5%AD%A6%E5%90%8E/</url>
    <content><![CDATA[<style>
  .markdown-body {
    font-family: Microsoft JhengHei, SimHei;
    font-size: 16px;
  }
</style>
<h1 align="center">
《放学后》
</h1>
<p>1.一口气看完了东野圭吾的《放学后》，作为一篇的悬疑推理小说，不论是情节的组织、细节的设计铺垫方面都是很不错的，也有东野圭吾一贯擅长的反转再反转，结局出人意料，回味前文的细节又觉得在情理之中，读完之后觉得深感震撼。</p>
<p>2.主角前岛是清华女中的一位数学老师，同时担任射箭社的指导老师，他不擅长上课，不擅长和学生互动，只是本分地完成自身的教学工作，被学生们叫做“机器”。就是这样一个不太和他人有交集以及过节的人，一次又一次险些遭人陷害，从天而降的花盆、泳池中的插电板、被人推向飞驰而过的列车……一次又一次险些丧命。这让他很是小心，急迫想要找出陷害他的人。而在他找到答案之前，同为数学老师的村桥老师在更衣室中被人杀害，警方介入调查。一波未平一波又起，在化装表演时，扮成小丑的竹井老师因为喝下道具酒瓶中的液体当场倒地身亡。</p>
<p>3.一切都是因为射箭社的惠子和惠美遭到了村桥和竹井的“视线强暴”，于是萌生了杀人的想法。她们用箭将男更衣室的门抵住，并且在男更衣室与女更衣室间隔的墙上留下痕迹，伪造了一幅凶手从女更衣室逃走的假象，误导警方断案。而在村桥的衣服中也发现了麻生老师的不雅照片，这是村桥老师逼迫麻生老师和他继续维持关系的砝码，后来却成了惠子和惠美威胁麻生协助杀害竹井老师的手段。那些让前岛一次又一次险些丧命的事件原来是她们伪造出来的假象，让警方误认为凶手想杀害的是前岛而不是竹井。</p>
<p>4.故事最后，前岛和妻子通过话后准备回家，却被一个陌生男子用刀捅死，故事戛然而止。委婉地叙述了妻子杀害前岛这一事实，让人大受震撼，但是转念回想前文，前岛和妻子意外有了孩子，前岛执意打掉了孩子，妻子因此也落寞了许久。这样结尾又在情理之中，妻子积怨已久，最终动手杀害了前岛。</p>
<p>5.一些书摘：</p>
<blockquote>
<p>成年人的案件倒不见得会那么复杂。报纸社会新闻版总有各种闹得沸沸扬扬的事件，几乎都能用色、欲、财这三要素来概括</p>
</blockquote>
<blockquote>
<p>但高中女生，就不能拿着几点来套了</p>
<p>对她们来说，最重要的应该是美丽、纯粹、真实的东西，比如友情、爱情，也可能是自己的身体或容貌。很多时候，更抽象的回忆或梦想对她们来说也很重要。反过来说，她们最憎恨企图破坏或从她们手中夺走这些重要东西的人</p>
</blockquote>
<p>6.处于青春期的孩子，自尊心是高于一切的，而成年人们最容易忽视的往往就是自尊心。有意无意间践踏了她们的自尊心却不自知，最终酿成悲剧。小说中还有一位女生叫阳子，她没有直接参与到几次谋杀案中。她母亲很早离世，父亲是位企业家，也无暇顾及女儿。阳子曾邀请前岛一同旅行，前岛最终是放了格子，阳子一人在车站苦苦等待了几个小时。在此之后，阳子也走向了堕落，同小混混飙车麻痹自己。没有人生来就是坏孩子，她们的成长也需要作为成年人的家长、老师的呵护。</p>
<p>7.主角前岛的死也是一出悲剧，忽视了妻子的感受，让负面的情绪在不断积攒，最终上演了悲剧。</p>
]]></content>
      <categories>
        <category>阅读</category>
      </categories>
      <tags>
        <tag>阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>白夜行</title>
    <url>/2022/09/04/%E7%99%BD%E5%A4%9C%E8%A1%8C/</url>
    <content><![CDATA[<style>
  .markdown-body {
    font-family: Microsoft JhengHei, SimHei;
    font-size: 16px;
  }
</style>
<h1 align="center">
《白夜行》
</h1>
<ol type="1">
<li><p>雪穗，生活在一个贫苦的单亲家庭，在母亲的策划下，即使还是个上小学的幼女却被迫出卖身体。</p></li>
<li><p>亮司，多次目睹母亲和雇佣的店员偷腥，甚至发现了父亲玷污自己的好朋友雪穗。</p></li>
<li><p>两人都有着极其不幸的童年，他们一同杀害了亮司的父亲，也间接谋害了雪穗的母亲，从此，雪穗投靠了亲戚步步向上走，而亮司也离开了母亲，一直生活在阴暗的世界里，帮助雪穗。</p></li>
<li><p>他们谋划了一起又一起犯罪事件，所有影响到雪穗人生的人物无一例外都遭到了他们的毒手。</p></li>
<li><p>最终他们都变成了自己最讨厌的模样：在雪穗的谋划下，自己女儿遭到了强暴，亮司正是强暴幼女的凶手。</p></li>
<li><p>不幸的童年造成了层出不穷的悲剧，但这并不是他们作恶多端的借口，他们不值得同情，那些他们手下的受害者又由谁去同情</p></li>
<li><blockquote>
<p>"我的人生就像在白夜里走路"</p>
</blockquote></li>
<li><blockquote>
<p>"一天当中，有太阳升起的时候，也有下沉的时候。人生也一样，有白天和黑夜，只是不会像真正的太阳那样，有定时的日出和日落。看个人，有些人一辈子都活在太阳的照耀下，也有些人不得不一直活在漆黑的深夜里。人害怕的，就是本来一直存在的太阳落下不再升起，也就是非常害怕原本照在身上的光芒消失"</p>
</blockquote></li>
</ol>
]]></content>
      <categories>
        <category>阅读</category>
      </categories>
      <tags>
        <tag>阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络</title>
    <url>/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<style> 
    .markdown-body {
      font-family: Microsoft JhengHei, SimHei;
      font-size: 16px;
    }
</style>
<h1 align="center">
神经网络
</h1>
<h2 id="监督学习supervised-learning">1.监督学习(Supervised
Learning)</h2>
<p>Learning from <font color="red">experience</font>(training data), and
build <font color="red">model</font> to <font color="red">predict</font> the
future</p>
<p>步骤：</p>
<ul>
<li>Collect training samples</li>
<li><font color="red">Define features</font></li>
<li><font color="red">Design &amp; Train Model</font></li>
<li>Make prediction</li>
</ul>
<h2 id="感知机perceptron">2.感知机(Perceptron)</h2>
<figure>
<img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/1.png" alt="1">
<figcaption aria-hidden="true">1</figcaption>
</figure>
<p>感知机用于对数据进行二分类，输出只有 1 和
-1，分别代表两种数据类别</p>
<p>对加权求和后的结果应用一个符号函数，若输入正数则输出1，否则输出-1</p>
<p>感知机模型的函数定义： <span class="math display">\[
f(x) = sign(\pmb{w}\pmb{x}+b)
\]</span> 其中： <span class="math display">\[
sign(x) = \left\{\begin{matrix} 1,x&gt;0 \\
-1,x&lt;0 \end{matrix}\right.
\]</span></p>
<p><img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/2.png" alt="2" style="zoom:50%;"></p>
<p>定义损失函数：</p>
<p>我们希望两边的数据点到超平面的距离总和越小越好</p>
<p>距离公式：(<span class="math inline">\(\pmb{w}\)</span>是超平面的法向量，该公式可以理解为$
cos$，即为点到平面距离) <span class="math display">\[
d = \frac{1}{||\pmb{w}||} |\pmb{w}\pmb{x}|
\]</span></p>
<p>ps：为什么<span class="math inline">\(\pmb{w}\)</span>是超平面<span class="math inline">\(\pmb{w}\pmb{x}=0\)</span>的法向量？</p>
<p>设<span class="math inline">\(\pmb{x_1}\)</span> ,<span class="math inline">\(\pmb{x_2}\)</span>是超平面内的两个不同向量 <span class="math display">\[
\pmb{w}\pmb{x_1}=0 \\
\pmb{w}\pmb{x_2}=0
\]</span> 相消后得 <span class="math display">\[
\pmb{w}(\pmb{x_1}-\pmb{x_2})=0
\]</span> 由于<span class="math inline">\(\pmb{x_1}-\pmb{x_2}\)</span>是超平面内的向量，于是<span class="math inline">\(\pmb{w}\)</span>是超平面的法向量</p>
<p>对于每一个误分类的点，均有： <span class="math display">\[
-y_i(\pmb{w}\pmb{x}_i+b) &gt; 0
\]</span> 可以借此去掉距离公式中的绝对值</p>
<p>ps：为什么不考虑<span class="math inline">\(\frac{1}{||\pmb{w}||}\)</span></p>
<p>因为感知机由最终的正负号来决定结果，去掉此部分不影响结果，且便于后续的求导。</p>
<p>最终得到损失函数： <span class="math display">\[
J(\pmb{w}) = -\sum_{i}\pmb{w}^T\pmb{x}_iy_i
\]</span> 求解梯度： <span class="math display">\[
\nabla J =-\sum_{i}\pmb{x}_iy_i
\]</span> 于是得到更新参数策略： <span class="math display">\[
\pmb{w}(k+1) = \pmb{w}(k) + \eta(k)\sum_{i}\pmb{x}_iy_i
\]</span></p>
<h2 id="deep-learning发展中遇到的问题">3.Deep
Learning发展中遇到的问题</h2>
<ul>
<li>很难训练一个深层次的网络(梯度消失问题)</li>
<li>计算资源耗费巨大</li>
<li>当时数据集较小，没有训练深度网络的需求</li>
</ul>
<h2 id="反向传播算法backpropagation-algorithm">4.反向传播算法(Backpropagation
Algorithm)</h2>
<p>过程如下：</p>
<figure>
<img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/5.png" alt="5">
<figcaption aria-hidden="true">5</figcaption>
</figure>
<h2 id="激活函数activation-function">5.激活函数(Activation
Function)</h2>
<p>一些特征与要求：</p>
<ul>
<li>必须是<font color="red">非线性</font>的而且<font color="red">有上下界</font>的</li>
<li>激活函数及其导数必须是<font color="red">连续的、光滑的、单调的</font></li>
<li>根据问题有针对性地选择</li>
</ul>
<blockquote>
<p><span class="math inline">\(sigmoid \space function \space(logistic
\space function)\)</span></p>
</blockquote>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1+e^{-x}}
\]</span></p>
<p><img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/3.png" alt="3" style="zoom:50%;"></p>
<blockquote>
<p><span class="math inline">\(tanh \space function\)</span></p>
</blockquote>
<p><span class="math display">\[
t(x) = \frac{e^x - e^{-x}}{e^{x}+e^{-x}} \\
t(x) = 2 \sigma(2x)-1
\]</span></p>
<p><img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/4.png" alt="4" style="zoom:50%;"></p>
<p>以tanh函数为例，其一阶导数形如：</p>
<figure>
<img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/6.png" alt="6">
<figcaption aria-hidden="true">6</figcaption>
</figure>
<p><strong><font color="red">可见其取值均小于1</font></strong></p>
<p><strong>反向传播时会出现<font color="red">梯度消失</font>问题！！！</strong></p>
<figure>
<img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/7.png" alt="7">
<figcaption aria-hidden="true">7</figcaption>
</figure>
<p>直到2011年，ReLU被提出</p>
<blockquote>
<p><span class="math inline">\(ReLU \space function\)</span></p>
</blockquote>
<p><span class="math display">\[
ReLU(x) = max(0,x)
\]</span></p>
<figure>
<img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/8.png" alt="8">
<figcaption aria-hidden="true">8</figcaption>
</figure>
<p>ReLU的优点：</p>
<ul>
<li>统计上有一半的神经元是静默的，ReLU更加<strong>贴合生物学规律</strong></li>
<li>ReLU相较于sigmoid通常具有<strong>更好的表现</strong>,且<strong>对预训练不敏感</strong></li>
<li><strong>计算资源的消耗更少</strong></li>
<li>ReLU的导数只能为0或1，<strong>不必担心梯度消失问题</strong></li>
<li>用<strong>更少</strong>激活的神经元就能灵活而稳定地表示特征</li>
</ul>
<h2 id="dropout-算法">6.Dropout 算法</h2>
<p>每个神经元以一定概率(e.g. p =
0.5)被激活，以减小大型神经网络的训练难度</p>
<figure>
<img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/9.png" alt="9">
<figcaption aria-hidden="true">9</figcaption>
</figure>
<h2 id="卷积神经网络cnnconvolutional-neural-networks">7.卷积神经网络(CNN,Convolutional
Neural Networks)</h2>
<p>图片中有大量的像素点</p>
<p>若是采用全连接神经网络将会需要巨量的参数</p>
<p><img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/10.png" alt="10" style="zoom:50%;"></p>
<p>图像具有很好的<strong>局部性</strong>(在时间上和空间上相近的像素高度相关)</p>
<p>CNN只连接一块一块的<strong>局部感受野</strong>，感受野中的像素<strong>共享权重</strong></p>
<p><img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/11.png" alt="11" style="zoom:50%;"></p>
<p>在数学上，卷积(Convolution)能够很好的反映两个函数之间的相似性 <span class="math display">\[
(f*g)(t) = \int_{-\infin}^{\infin}f(\tau)g(t-\tau)d\tau
\]</span></p>
<p>对图像的卷积操作(利用卷积核)</p>
<p><img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/12.png" alt="12" style="zoom:50%;"></p>
<p>不同的卷积核会带来不同的效果</p>
<figure>
<img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/13.png" alt="13">
<figcaption aria-hidden="true">13</figcaption>
</figure>
<p>CNN的一般结构</p>
<figure>
<img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/14.png" alt="14">
<figcaption aria-hidden="true">14</figcaption>
</figure>
<p><strong>有时可以没有池化层和全连接层</strong></p>
<p>池化(Pooling)</p>
<p>举个例子,MAX POOLING</p>
<figure>
<img src="/2022/09/20/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/15.png" alt="15">
<figcaption aria-hidden="true">15</figcaption>
</figure>
<p>取每个区域内的最大值作为新的值</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>线性回归</title>
    <url>/2022/09/19/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-1/</url>
    <content><![CDATA[<style> 
    .markdown-body {
      font-family: Microsoft JhengHei, SimHei;
      font-size: 16px;
    }
</style>
<h1 align="center">
线性回归
</h1>
<h2 id="分类classification与回归regression">1.分类(Classification)与回归(Regression)</h2>
<p>共同点：</p>
<ul>
<li>学习一个从<code>输入x</code>到<code>输出y</code>的<font color="red">映射</font></li>
</ul>
<p>分类：</p>
<ul>
<li>y是一个分类变量</li>
</ul>
<p>回归：</p>
<ul>
<li>y是一个实际的数值</li>
</ul>
<h2 id="线性模型linear-model">2.线性模型(Linear Model)</h2>
<p>Linear Function: <span class="math display">\[
f(x) = \pmb{w}^T\pmb{x} + b
\]</span></p>
<blockquote>
<p><span class="math inline">\(\pmb{x} = [x_1,x_2,...,x_d]^T \in
R^d\)</span></p>
<p><span class="math inline">\(\pmb{w} = [w_1,w_2,...,w_d]^T \in
R^d\)</span></p>
</blockquote>
<p>进一步简化表示为： <span class="math display">\[
f(x) = \pmb{w}^T\pmb{x}
\]</span></p>
<blockquote>
<p><span class="math inline">\(\pmb{x} = [x_1,x_2,...,x_d,1]^T \in
R^{d+1}\)</span></p>
<p><span class="math inline">\(\pmb{w} = [w_1,w_2,...,w_d,b]^T \in
R^{d+1}\)</span></p>
</blockquote>
<h2 id="多项式曲线拟合polynomial-curve-fitting">3.多项式曲线拟合(Polynomial
Curve Fitting)</h2>
<p>用多项式去拟合若干给定的数据点</p>
<figure>
<img src="/2022/09/19/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-1/1.png" alt="1">
<figcaption aria-hidden="true">1</figcaption>
</figure>
<p><span class="math display">\[
f(x,\pmb{w}) = w_0 + w_1x + w_2x + ... +w_Mx^M = \sum_{j=0}^{M}w_jx^j
\]</span></p>
<p>一些例子:</p>
<p><span class="math inline">\(0^{th} Order Polynomial\)</span></p>
<figure>
<img src="/2022/09/19/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-1/2.png" alt="2">
<figcaption aria-hidden="true">2</figcaption>
</figure>
<p><span class="math inline">\(1^{st} Order Polynomial\)</span></p>
<figure>
<img src="/2022/09/19/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-1/3.png" alt="3">
<figcaption aria-hidden="true">3</figcaption>
</figure>
<p><span class="math inline">\(3^{rd} Order Polynomial\)</span></p>
<figure>
<img src="/2022/09/19/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-1/4.png" alt="4">
<figcaption aria-hidden="true">4</figcaption>
</figure>
<p>多项式拟合是一种线性模型</p>
<blockquote>
<p><span class="math inline">\(\pmb{x} =
[1,x,x^2,...,x^M]^T\)</span></p>
<p><span class="math inline">\(\pmb{w} =
[w_1,w_2,...,w_M]^T\)</span></p>
<p><span class="math inline">\(f(x,\pmb{w}) =
\pmb{w}^T\pmb{x}\)</span></p>
</blockquote>
<h2 id="损失函数error-function">4.损失函数(Error Function)</h2>
<figure>
<img src="/2022/09/19/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-1/5.png" alt="5">
<figcaption aria-hidden="true">5</figcaption>
</figure>
<blockquote>
<p>空心点为训练数据(training data)</p>
<p>实线为训练所得函数 <span class="math inline">\(f(x) = y\)</span></p>
</blockquote>
<p>均方差损失函数(MSE,Mean Square Error) <span class="math display">\[
MSE(\pmb{w})=\frac{1}{n}\sum_{i=1}^{n}(y_i-f(x_i,\pmb{w}))^2
\]</span> 用来评价训练所得函数与训练数据的吻合程度</p>
<p>训练目标是<font color="red">最小化</font>均方差损失(n可以看作一个常数暂时抛开不看)</p>
<p>化简得： <span class="math display">\[
J_n = \sum_{i=1}^{n}(y_i - \pmb{w}^T\pmb{x}_i)^2
\]</span> 使用矩阵相关记号:</p>
<blockquote>
<p><span class="math inline">\(X =
[\pmb{x}_1,...,\pmb{x}_n]\)</span></p>
<p><span class="math inline">\(y =
[\pmb{y}_1,...,\pmb{y}_n]^T\)</span></p>
</blockquote>
<p><span class="math display">\[
J_n(\pmb{w})=(\pmb{y}-X^T\pmb{w})^T(\pmb{y}-X^T\pmb{w})
\]</span></p>
<p>梯度为0时损失值下降最快</p>
<p>先计算梯度： <span class="math display">\[
\nabla J_n = -2X(\pmb{y}-X^T\pmb{w})
\]</span> 令梯度为0： <span class="math display">\[
XX^T \pmb{w} = X \pmb{y} \\
\pmb{w} = (XX^T)^{-1}X\pmb{y}
\]</span> 于是得到了参数<span class="math inline">\(\pmb{w}\)</span>的修正后的数值</p>
<h2 id="统计模型statistical-model">5.统计模型(Statistical Model)</h2>
<p>给定一个生成模型: <span class="math display">\[
y = f(\pmb{x},\pmb{w}) + \epsilon
\]</span> 假设: <span class="math display">\[
\epsilon  \sim N(0,\sigma^2)
\]</span> 于是: <span class="math display">\[
p(y|\pmb{x},\pmb{w},\sigma) = \frac{1}{\sigma \sqrt{2\pi}}
e^{-\frac{1}{2\sigma^2}}(y-f(\pmb{x},\pmb{w}))^2
\]</span></p>
<p>先介绍一下argmax函数</p>
<blockquote>
<p>有一个函数 <span class="math inline">\(y = f(x)\)</span> 时</p>
<p>若有 <span class="math inline">\(x_0 = argmax(f(x))\)</span></p>
<p>则表示当 <span class="math inline">\(x = x_0\)</span> 时，函数 <span class="math inline">\(f(x)\)</span> 取到最大值</p>
</blockquote>
<p>极大似然估计(Maximum Likelihood Estimation) <span class="math display">\[
L(\pmb{D},\pmb{w},\sigma)=\prod_{i=1}^{n}
p(y_i|\pmb{x}_i,\pmb{w},\sigma) \\
\pmb{w}^* = argmax \prod_{i=1}^{n} p(y_i|\pmb{x}_i,\pmb{w},\sigma)
\]</span> 由于log函数单调不影响结果,于是取对数为Log-likelihood <span class="math display">\[
l(\pmb{D},\pmb{w},\sigma) = log(L(\pmb{D},\pmb{w},\sigma)) =
log\prod_{i=1}^{n} p(y_i|\pmb{x}_i,\pmb{w},\sigma) \\
=\sum_{i=1}^{n}log \space p(y_i|\pmb{x}_i,\pmb{w},\sigma)\\
= -\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y-f(\pmb{x,\pmb{w}}))^2+c(\sigma)
\]</span> 忽略常数后得到: <span class="math display">\[
RSS(f) = \sum_{i=1}^{n}(y_i-f(\pmb{x}_i,\pmb{w}))^2
\]</span></p>
<p><strong>线性回归模型 <span class="math inline">\(\Leftrightarrow\)</span>
具有高斯噪声的生成模型的极大似然估计</strong></p>
<hr>
<p>为了解决模型过拟合(Over-fitting)的问题，在原来基础上加入惩罚项，参数越多(模型越复杂)惩罚越大。</p>
<hr>
<h2 id="岭回归ridge-regressionl2正则化">6.岭回归(Ridge
Regression)(L2正则化)</h2>
<p><span class="math display">\[
w^* = argmin\sum_{i=1}^{n}(y_i-\pmb{x}_i^T\pmb{w})^2  \textcolor{red}{ +
\lambda\sum^{p}_{j=1}w_j^2}
\]</span></p>
<p>其对参数的限制方式可以表示为: <span class="math display">\[
\textcolor{red}{\sum^{p}_{j=1}w_j^2 \le t}
\]</span></p>
<p>经过矩阵表示、梯度计算、化简等步骤后得到: <span class="math display">\[
\pmb{w}^* = (XX^T+\lambda\pmb{I})^{-1}X\pmb{y}
\]</span> 可以人为调节<span class="math inline">\(\lambda\)</span>的大小来得到更好效果的模型参数</p>
<h2 id="lasso回归least-absolute-selection-and-shrinkage-operatorl1正则化">7.LASSO回归(Least
Absolute Selection and Shrinkage Operator)(L1正则化)</h2>
<p><span class="math display">\[
w^* = argmin\sum_{i=1}^{n}(y_i-\pmb{x}_i^T\pmb{w})^2  \textcolor{red}{ +
\lambda\sum^{p}_{j=1}|w_j|}
\]</span></p>
<p>其对参数的限制方式为: <span class="math display">\[
\textcolor{red}{\sum^{p}_{j=1}|w_j| \le t}
\]</span></p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>贝叶斯决策理论</title>
    <url>/2022/09/13/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E7%90%86%E8%AE%BA/</url>
    <content><![CDATA[<style>
  .markdown-body {
    font-family: Microsoft JhengHei, SimHei;
    font-size: 16px;
  }
</style>
<h1 align="center">
贝叶斯决策理论
</h1>
<h2 id="basics-of-probability概率论基础知识">1. Basics of
Probability(概率论基础知识)</h2>
<ul>
<li>$ A独立于B时,P(A|B) = P(A) $</li>
<li>$ P(A|B)= $</li>
<li>$ A、B相互独立 当且仅当 P(A,B)=P(A)P(B) $</li>
</ul>
<h2 id="bayes-theorem贝叶斯理论">2. Bayes' Theorem(贝叶斯理论)</h2>
<p><span class="math display">\[
P(\omega|x) = \frac{p(x|\omega)p(\omega)}{p(x)}\\
Posterior = (Likelihood \times Prior) / Evidence\\
p(x) = \sum_{i=1}^{k}p(x|\omega_i)p(\omega_i)
\]</span></p>
<ul>
<li><span class="math inline">\(P(\omega|x)\)</span> :
posterior,后验概率,<font color="red">已发生事件</font>出于某种因素引发的概率</li>
<li><span class="math inline">\(p(x|\omega)\)</span> :
likelihood,似然,数据符合<font color="red">某种分布</font>的概率大小</li>
<li><span class="math inline">\(p(\omega)\)</span> :
prior,先验概率,<font color="red">未发生事件</font>依据以往经验得知的概率</li>
<li><span class="math inline">\(p(x)\)</span> :
evidence,往往作为一个<font color="red">常数</font></li>
</ul>
<blockquote>
<p>其中 x为样本</p>
</blockquote>
<h2 id="optimal-bayes-decision-rule最优贝叶斯决策规则">3. Optimal Bayes
Decision Rule(最优贝叶斯决策规则)</h2>
<p><strong>一个物体选择后验概率大的作为其分类可以最小化决策误差</strong></p>
<p><span class="math inline">\(P(\omega_1|x) &gt;
P(\omega_2|x)\)</span>时认为它是<span class="math inline">\(\omega_1\)</span></p>
<p><span class="math inline">\(P(\omega_1|x) &lt;
P(\omega_2|x)\)</span>时认为它是<span class="math inline">\(\omega_2\)</span></p>
<blockquote>
<p>同理或者选择风险R小的</p>
</blockquote>
<p>特殊情况：</p>
<ul>
<li><p><span class="math inline">\(p(\omega_1)=p(\omega_2)\)</span> :
<span class="math inline">\(p(x|\omega_1)&gt;p(x|\omega_2)\)</span>时认为是<span class="math inline">\({\omega}_1\)</span></p>
<blockquote>
<p>最大似然决策法</p>
<p>本质上和一般情况没有区别，只是先验概率相同可以同时约去，不加入比较，直接比较似然度</p>
</blockquote></li>
<li><p><span class="math inline">\(p(x|\omega_1)=p(x|\omega_2)\)</span>
: <span class="math inline">\(p(\omega_1)&gt;p(\omega_2)\)</span>时认为是<span class="math inline">\({\omega}_1\)</span></p>
<blockquote>
<p>同上</p>
</blockquote></li>
</ul>
<h2 id="bayes-risk">4. Bayes Risk</h2>
<p>Conditional Risk <span class="math display">\[
R(\alpha_i|x)=\sum_{j=1}^{c}\lambda(\alpha_i|\omega_j)P(\omega_j|x)
\]</span></p>
<blockquote>
<p><span class="math inline">\(\alpha\)</span> 代表 采取的行为</p>
</blockquote>
<p>Overall Risk <span class="math display">\[
R = \int R(\alpha_i|x)p(x)dx
\]</span> Bayes Risk</p>
<ul>
<li>风险最小时的R</li>
</ul>
<h2 id="example-1two-category-classification">Example 1：Two-category
classification</h2>
<p>给定： <span class="math display">\[
\alpha_1 = deciding \space \omega_1 \\
\alpha_2 = deciding \space \omega_2 \\
\lambda_{ij} = \lambda(\alpha_i | \omega_j)
\]</span> 于是条件风险： <span class="math display">\[
R(\alpha_1 | x) = \lambda_{11}P(\omega_1 | x) + \lambda_{12}P(\omega_2 |
x) \\
R(\alpha_2 | x) = \lambda_{21}P(\omega_1 | x) + \lambda_{22}P(\omega_2 |
x)
\]</span> 如何找到贝叶斯风险？？？</p>
<p>认为是<span class="math inline">\({\omega}_1\)</span>当 <span class="math display">\[
R(\alpha_1 | x) &lt; R(\alpha_2 | x)
\]</span> 展开并移项化简得到(认为是<span class="math inline">\({\omega}_1\)</span>的情况) <span class="math display">\[
\frac{P(x|\omega_1)}{P(x|\omega_2)} &gt;
\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}} \times
\frac{P(\omega_2)}{P(\omega_1)}
\]</span></p>
<h2 id="example-2-minimum-error-rate-classification">Example 2:
Minimum-Error-Rate Classification</h2>
<p>给定</p>
<ul>
<li>对于动作<span class="math inline">\(\alpha_i\)</span>和事实上的类别<span class="math inline">\(\omega_j\)</span> ,如果<span class="math inline">\(i=j\)</span>就是正确的，否则错误</li>
</ul>
<p>找到最小损失的分类</p>
<p>写出损失函数(Zero-one loss function): <span class="math display">\[
\lambda(\alpha_i | \omega_j) =
\begin{cases}
0&amp; \text{i = j}\\
1&amp; \text{i} \neq \text{j}
\end{cases}
\]</span> 于是可以计算条件风险 <span class="math display">\[
R(\alpha_i|x) = \sum_{j=1}^{c} \lambda(\alpha_i|\omega_j)P(\omega_j | x)
\\
= \sum_{j{\neq}i} P(\omega_j | x) = 1-P(\omega_i | x)
\]</span>
需要风险最小化，由上述式子可知需要后验概率最大化，选择后验概率最大的即可</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
